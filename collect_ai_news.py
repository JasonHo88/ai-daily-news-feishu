#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ”¶é›†è„šæœ¬ï¼ˆå¤šæº RSS æ——èˆ°ç‰ˆï¼‰
1. å›½å†…æºï¼š36Kr, ITä¹‹å®¶, å°‘æ•°æ´¾, æå®¢å…¬å›­, è™å—… (å…¨ RSS é©±åŠ¨)
2. å›½å¤–æºï¼šAI News, TechCrunch (å…¨è‡ªåŠ¨ä¸­æ–‡ç¿»è¯‘)
3. æ ¸å¿ƒæœºåˆ¶ï¼šå¤šæºå‡è¡¡ã€æŒ‡çº¹å»é‡ã€åœ¨çº¿ç¿»è¯‘
"""

import os
import sys
import re
from datetime import datetime
from typing import Dict, List

# ä¾èµ–åº“è‡ªåŠ¨å®‰è£…
def install_dependencies():
    needed = ['requests', 'feedparser', 'deep-translator']
    for lib in needed:
        try:
            __import__(lib.replace('-', '_'))
        except ImportError:
            print(f"âŒ æ­£åœ¨å®‰è£… {lib}...")
            import subprocess
            subprocess.run([sys.executable, "-m", "pip", "install", lib, "-q"])

install_dependencies()

import requests
import feedparser
from deep_translator import GoogleTranslator

# --- ç¯å¢ƒé…ç½® ---
FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL', '')
TODAY = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

# åˆå§‹åŒ–ç¿»è¯‘å™¨ (è‹±ç¿»ä¸­)
translator = GoogleTranslator(source='auto', target='zh-CN')

class NewsEngine:
    def __init__(self):
        self.seen_titles = set()

    def translate(self, text: str) -> str:
        if not text: return ""
        try:
            # ç¿»è¯‘æ ‡é¢˜ï¼Œä¿ç•™ä¸€äº›ä¸“ä¸šæœ¯è¯­ä¸è¢«è¯¯ç¿»
            return translator.translate(text)
        except: return text

    def is_dup(self, title: str) -> bool:
        """æ ¹æ®æ ‡é¢˜å‰15ä¸ªå­—ç¬¦è¿›è¡Œç®€æ˜“æŒ‡çº¹å»é‡"""
        clean = re.sub(r'[^\w\u4e00-\u9fa5]', '', title.lower())[:15]
        if not clean or clean in self.seen_titles: return True
        self.seen_titles.add(clean)
        return False

# --- æŠ“å–é€»è¾‘ ---

def fetch_domestic_rss(engine: NewsEngine) -> List[Dict]:
    """èšåˆå›½å†…å¤šä¸ªç§‘æŠ€åª’ä½“ RSS"""
    sources = [
        {"name": "36æ°ª", "url": "https://36kr.com/feed-article"},
        {"name": "ITä¹‹å®¶", "url": "https://www.ithome.com/rss/"},
        {"name": "å°‘æ•°æ´¾", "url": "https://sspai.com/feed"},
        {"name": "æå®¢å…¬å›­", "url": "http://www.geekpark.net/rss"},
        {"name": "è™å—…", "url": "https://www.huxiu.com/rss/0.xml"}
    ]
    results = []
    
    for src in sources:
        try:
            print(f"ğŸ‡¨ğŸ‡³ æ­£åœ¨åŒæ­¥ {src['name']}...")
            feed = feedparser.parse(src['url'])
            # æ¯ä¸ªæºå–å‰ 2-3 æ¡æœ€åŠæ—¶çš„ï¼Œä¿æŒæ—¥æŠ¥ç´§å‡‘
            count = 0
            for entry in feed.entries:
                if count >= 3: break
                if not engine.is_dup(entry.title):
                    results.append({
                        "title": entry.title,
                        "source": src['name'],
                        "link": entry.link
                    })
                    count += 1
        except Exception as e:
            print(f"âš ï¸ {src['name']} è®¿é—®å—é™: {e}")
            
    return results

def fetch_overseas_rss(engine: NewsEngine) -> List[Dict]:
    """æŠ“å–æµ·å¤–æºå¹¶ç¿»è¯‘"""
    sources = [
        {"name": "AI News", "url": "https://www.artificialintelligence-news.com/feed/"},
        {"name": "TechCrunch", "url": "https://techcrunch.com/category/artificial-intelligence/feed/"}
    ]
    results = []
    for src in sources:
        try:
            print(f"ğŸŒ æ­£åœ¨æŠ“å–å¹¶ç¿»è¯‘ {src['name']}...")
            feed = feedparser.parse(src['url'])
            for entry in feed.entries[:4]:
                if not engine.is_dup(entry.title):
                    results.append({
                        "title": engine.translate(entry.title),
                        "source": src['name'],
                        "link": entry.link
                    })
        except: pass
    return results

# --- æ¨é€é€»è¾‘ ---

def main():
    engine = NewsEngine()
    print("=" * 30)
    domestic = fetch_domestic_rss(engine)
    overseas = fetch_overseas_rss(engine)
    
    if not domestic and not overseas:
        print("âŒ å…¨ç½‘èµ„è®¯è¿æ¥å¤±è´¥")
        return

    # æ„å»ºé£ä¹¦æ¶ˆæ¯ä½“
    report = f"# ğŸ¤– AI & ç§‘æŠ€å…¨ç½‘èšåˆæ—¥æŠ¥ - {TODAY}\n\n"
    
    report += "## ğŸŒ æµ·å¤–å‰æ²¿ (æ™ºèƒ½ç¿»è¯‘)\n\n"
    for i, n in enumerate(overseas[:8], 1):
        report += f"**{i}. {n['title']}**\n- æ¥æº: {n['source']} | [åŸæ–‡é“¾æ¥]({n['link']})\n\n"
    
    report += "## ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€ (å¤šæºèšåˆ)\n\n"
    for i, n in enumerate(domestic[:10], 1):
        report += f"**{i}. {n['title']}**\n- æ¥æº: {n['source']} | [é˜…è¯»å…¨æ–‡]({n['link']})\n\n"
    
    report += f"---\n*æƒ…æŠ¥è¦†ç›–: 36Kr, ITä¹‹å®¶, å°‘æ•°æ´¾, æå®¢å…¬å›­, è™å—…, AI News, TechCrunch*"

    if FEISHU_WEBHOOK_URL:
        payload = {
            "msg_type": "interactive",
            "card": {
                "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– å…¨çƒAIç§‘æŠ€æ—¥æŠ¥ - {TODAY}"}, "template": "purple"},
                "elements": [{"tag": "markdown", "content": report}]
            }
        }
        requests.post(FEISHU_WEBHOOK_URL, json=payload, timeout=20)
        print("âœ… æ—¥æŠ¥æ¨é€æˆåŠŸï¼")
    else:
        print("\n--- é¢„è§ˆå†…å®¹ ---\n")
        print(report)

if __name__ == "__main__":
    main()
