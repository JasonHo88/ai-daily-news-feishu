#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ”¶é›†è„šæœ¬ï¼ˆå¤šæº+å…¨ä¸­æ–‡ä¿®å¤ç‰ˆï¼‰
1. è‡ªåŠ¨ç¿»è¯‘æµ·å¤–èµ„è®¯ä¸ºä¸­æ–‡
2. ä¿®å¤å¹¶å¢å¼ºå›½å†…æŠ“å–æº
3. èšåˆå»é‡ä¸é£ä¹¦æ¨é€
"""

import os
import sys
import re
from datetime import datetime
from typing import Dict, List

# è‡ªåŠ¨å®‰è£…å¿…è¦åº“
def install_dependencies():
    needed = ['requests', 'beautifulsoup4', 'feedparser', 'deep-translator']
    for lib in needed:
        try:
            if lib == 'beautifulsoup4': __import__('bs4')
            elif lib == 'deep-translator': __import__('deep_translator')
            else: __import__(lib)
        except ImportError:
            print(f"âŒ ç¼ºå°‘ {lib}ï¼Œæ­£åœ¨å®‰è£…...")
            import subprocess
            subprocess.run([sys.executable, "-m", "pip", "install", lib, "-q"])

install_dependencies()

import requests
from bs4 import BeautifulSoup
import feedparser
from deep_translator import GoogleTranslator

# --- é…ç½®åŒº ---
FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL', '')
TODAY = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# åˆå§‹åŒ–ç¿»è¯‘å™¨
translator = GoogleTranslator(source='auto', target='zh-CN')

# --- è¾…åŠ©åŠŸèƒ½ ---
class ContentProcessor:
    def __init__(self):
        self.seen_titles = set()

    def translate(self, text: str) -> str:
        """è‡ªåŠ¨ç¿»è¯‘ä¸ºä¸­æ–‡"""
        if not text: return ""
        try:
            return translator.translate(text)
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
            return text

    def is_duplicate(self, title: str) -> bool:
        clean_title = re.sub(r'[^\w\u4e00-\u9fa5]', '', title.lower())
        fingerprint = clean_title[:15]
        if fingerprint in self.seen_titles: return True
        self.seen_titles.add(fingerprint)
        return False

# --- æŠ“å–é€»è¾‘ ---

def fetch_overseas_v3(processor: ContentProcessor) -> List[Dict]:
    """æŠ“å–æµ·å¤–æºå¹¶ç¿»è¯‘"""
    sources = [
        {"name": "AI News", "url": "https://www.artificialintelligence-news.com/feed/"},
        {"name": "TechCrunch", "url": "https://techcrunch.com/category/artificial-intelligence/feed/"}
    ]
    results = []
    for s in sources:
        try:
            print(f"ğŸŒ æ­£åœ¨è·å–å¹¶ç¿»è¯‘ {s['name']}...")
            feed = feedparser.parse(s['url'])
            for entry in feed.entries[:4]:
                raw_title = entry.title
                if processor.is_duplicate(raw_title): continue
                
                # æ‰§è¡Œç¿»è¯‘
                zh_title = processor.translate(raw_title)
                results.append({
                    "title": zh_title,
                    "source": s['name'],
                    "link": entry.link
                })
        except Exception as e:
            print(f"âš ï¸ æµ·å¤–æº {s['name']} å¼‚å¸¸: {e}")
    return results

def fetch_domestic_v3(processor: ContentProcessor) -> List[Dict]:
    """ä¿®å¤å›½å†…æŠ“å–é€»è¾‘"""
    results = []
    # ç­–ç•¥ï¼šå¦‚æœ 36Kr å¤±è´¥ï¼Œè‡ªåŠ¨å°è¯• ITä¹‹å®¶
    try:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨å°è¯• 36Kr...")
        res = requests.get("https://36kr.com/information/ai/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        # å…¼å®¹å¤šç§å¯èƒ½çš„ 36Kr æ ‡é¢˜ç±»å
        items = soup.find_all('a', class_=re.compile(r'article-item-title|weight'))
        for item in items[:10]:
            title = item.get_text(strip=True)
            if title and not processor.is_duplicate(title):
                link = item['href'] if item['href'].startswith('http') else f"https://36kr.com{item['href']}"
                results.append({"title": title, "source": "36æ°ª", "link": link})
    except Exception as e:
        print(f"âš ï¸ 36Kr è§£æå¤±è´¥: {e}")

    try:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨å°è¯• ITä¹‹å®¶...")
        res = requests.get("https://www.ithome.com/tag/ai", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('.news-item .title')
        for item in items[:8]:
            title = item.get_text(strip=True)
            if title and not processor.is_duplicate(title):
                results.append({"title": title, "source": "ITä¹‹å®¶", "link": item['href']})
    except Exception as e:
        print(f"âš ï¸ ITä¹‹å®¶ è§£æå¤±è´¥: {e}")
        
    return results

# --- ä¸»ç¨‹åº ---

def main():
    processor = ContentProcessor()
    
    overseas = fetch_overseas_v3(processor)
    domestic = fetch_domestic_v3(processor)
    
    if not overseas and not domestic:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– Secret é…ç½®ã€‚")
        return

    # æ„é€ é£ä¹¦å¡ç‰‡å†…å®¹
    report = f"# ğŸ¤– AI å…¨ç½‘èšåˆæ—¥æŠ¥ - {TODAY}\n\n"
    
    report += "## ğŸ“° æµ·å¤–çƒ­ç‚¹ (å·²ç¿»è¯‘)\n\n"
    for i, n in enumerate(overseas[:6], 1):
        report += f"**{i}. {n['title']}**\n- æ¥æº: {n['source']} | [æŸ¥çœ‹è¯¦æƒ…]({n['link']})\n\n"
    
    report += "## ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€\n\n"
    if not domestic:
        report += "_âš ï¸ å›½å†…èµ„è®¯æŠ“å–æš‚æ—¶å—é™ï¼Œæ­£åœ¨ä¿®å¤ä¸­_\n\n"
    for i, n in enumerate(domestic[:6], 1):
        report += f"**{i}. {n['title']}**\n- æ¥æº: {n['source']} | [æŸ¥çœ‹è¯¦æƒ…]({n['link']})\n\n"
    
    report += f"---\n*Matrix Agent æ™ºèƒ½èšåˆç¿»è¯‘ç‰ˆ | {TODAY}*"

    # æ¨é€é€»è¾‘
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– AIæ—¥æŠ¥ (å…¨ä¸­æ–‡ç‰ˆ) - {TODAY}"}, "template": "blue"},
            "elements": [{"tag": "markdown", "content": report}]
        }
    }
    
    if FEISHU_WEBHOOK_URL:
        requests.post(FEISHU_WEBHOOK_URL, json=payload, timeout=20)
        print("âœ… å…¨ä¸­æ–‡æ—¥æŠ¥æ¨é€å®Œæˆï¼")
    else:
        print("âš ï¸ æœªå‘ç° Webhook åœ°å€ï¼Œæ— æ³•æ¨é€ã€‚")

if __name__ == "__main__":
    main()
