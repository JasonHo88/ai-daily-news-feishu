#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ”¶é›†è„šæœ¬ï¼ˆå…¨ä¸­æ–‡+å¤šæºä¿åº•ç‰ˆï¼‰
1. è‡ªåŠ¨ç¿»è¯‘æµ·å¤–æº 
2. å›½å†…æ¥å…¥ï¼š36Kr + ç•Œé¢æ–°é—» + æ–°æµªç§‘æŠ€ï¼ˆå¤šæºäº’è¡¥ï¼‰
"""

import os
import sys
import re
from datetime import datetime
from typing import Dict, List

# ä¾èµ–åº“è‡ªåŠ¨å®‰è£…
def install_dependencies():
    needed = ['requests', 'beautifulsoup4', 'feedparser', 'deep-translator']
    for lib in needed:
        try:
            if lib == 'beautifulsoup4': __import__('bs4')
            else: __import__(lib.replace('-', '_'))
        except ImportError:
            print(f"âŒ æ­£åœ¨å®‰è£… {lib}...")
            import subprocess
            subprocess.run([sys.executable, "-m", "pip", "install", lib, "-q"])

install_dependencies()

import requests
from bs4 import BeautifulSoup
import feedparser
from deep_translator import GoogleTranslator

# --- ç¯å¢ƒé…ç½® ---
FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL', '')
TODAY = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
}

translator = GoogleTranslator(source='auto', target='zh-CN')

class NewsEngine:
    def __init__(self):
        self.seen_titles = set()

    def translate(self, text: str) -> str:
        if not text: return ""
        try:
            return translator.translate(text)
        except: return text

    def is_dup(self, title: str) -> bool:
        clean = re.sub(r'[^\w\u4e00-\u9fa5]', '', title.lower())[:12]
        if clean in self.seen_titles: return True
        self.seen_titles.add(clean)
        return False

# --- å›½å†…æºæŠ“å– (å¤šæºäº’è¡¥) ---

def fetch_domestic(engine: NewsEngine) -> List[Dict]:
    results = []
    
    # æ¥æº1: ç•Œé¢æ–°é—» (AIé¢‘é“ - ç¨³å®šæ€§é«˜)
    try:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨æŠ“å– ç•Œé¢æ–°é—»...")
        res = requests.get("https://www.jiemian.com/lists/211.html", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('.news-view .news-header a')
        for item in items[:5]:
            title = item.get_text(strip=True)
            if not engine.is_dup(title):
                results.append({"title": title, "source": "ç•Œé¢æ–°é—»", "link": item['href']})
    except: pass

    # æ¥æº2: 36Kr (ä¿®å¤åçš„é€‰æ‹©å™¨)
    try:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨å°è¯• 36Kr...")
        res = requests.get("https://36kr.com/information/ai/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        # é”å®šæ–‡ç« ä¿¡æ¯æµåŒºåŸŸï¼Œé¿å¼€å¯¼èˆªæ 
        items = soup.select('a.article-item-title-weight, .kr-flow-article-item a.article-item-title')
        for item in items[:5]:
            title = item.get_text(strip=True)
            if title and len(title) > 5 and not engine.is_dup(title):
                link = item['href'] if item['href'].startswith('http') else f"https://36kr.com{item['href']}"
                results.append({"title": title, "source": "36æ°ª", "link": link})
    except: pass

    return results

# --- æµ·å¤–æºæŠ“å– (å¸¦ç¿»è¯‘) ---

def fetch_overseas(engine: NewsEngine) -> List[Dict]:
    sources = [
        {"name": "AI News", "url": "https://www.artificialintelligence-news.com/feed/"},
        {"name": "TechCrunch", "url": "https://techcrunch.com/category/artificial-intelligence/feed/"}
    ]
    results = []
    for s in sources:
        try:
            print(f"ğŸŒ æ­£åœ¨è·å–å¹¶ç¿»è¯‘ {s['name']}...")
            feed = feedparser.parse(s['url'])
            for entry in feed.entries[:4]:
                raw_title = entry.title
                if not engine.is_dup(raw_title):
                    results.append({
                        "title": engine.translate(raw_title),
                        "source": s['name'],
                        "link": entry.link
                    })
        except: pass
    return results

# --- æ‰§è¡Œä¸æ¨é€ ---

def main():
    engine = NewsEngine()
    overseas = fetch_overseas(engine)
    domestic = fetch_domestic(engine)
    
    if not overseas and not domestic:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
        return

    report = f"# ğŸ¤– AIå…¨ç½‘ä¸­æ–‡æ—¥æŠ¥ - {TODAY}\n\n"
    
    report += "## ğŸ“° æµ·å¤–çƒ­ç‚¹ (ç¿»è¯‘ç‰ˆ)\n\n"
    for i, n in enumerate(overseas[:6], 1):
        report += f"**{i}. {n['title']}**\n- æ¥æº: {n['source']} | [åŸæ–‡é“¾æ¥]({n['link']})\n\n"
    
    report += "## ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€ (å¤šæºç²¾é€‰)\n\n"
    if not domestic:
        report += "_âš ï¸ å›½å†…æºè¿æ¥ä¸­ï¼Œå»ºè®®ç¨åé‡è¯•_\n\n"
    for i, n in enumerate(domestic[:6], 1):
        report += f"**{i}. {n['title']}**\n- æ¥æº: {n['source']} | [æŸ¥çœ‹è¯¦æƒ…]({n['link']})\n\n"
    
    report += f"---\n*Matrix Agent è‡ªåŠ¨èšåˆç¿»è¯‘ | {TODAY}*"

    # å‘é€é£ä¹¦
    if FEISHU_WEBHOOK_URL:
        payload = {
            "msg_type": "interactive",
            "card": {
                "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– AIæ—¥æŠ¥ (å…¨ä¸­æ–‡ç‰ˆ) - {TODAY}"}, "template": "blue"},
                "elements": [{"tag": "markdown", "content": report}]
            }
        }
        requests.post(FEISHU_WEBHOOK_URL, json=payload, timeout=20)
        print("âœ… æ¨é€æˆåŠŸï¼")

if __name__ == "__main__":
    main()
