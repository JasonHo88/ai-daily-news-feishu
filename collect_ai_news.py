#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ”¶é›†è„šæœ¬ï¼ˆå¤šæºèšåˆç‰ˆï¼‰
èšåˆå›½å†…å¤–å¤šä¸ªæƒå¨æºï¼Œæ”¯æŒè‡ªåŠ¨å»é‡ä¸å®¹é”™
"""

import os
import sys
import time
import re
from datetime import datetime
from typing import Dict, List

# è‡ªåŠ¨æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ç¬¬ä¸‰æ–¹åº“
def install_dependencies():
    needed = ['requests', 'beautifulsoup4', 'feedparser']
    for lib in needed:
        try:
            __import__(lib if lib != 'beautifulsoup4' else 'bs4')
        except ImportError:
            print(f"âŒ ç¼ºå°‘ {lib} åº“ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
            import subprocess
            subprocess.run([sys.executable, "-m", "pip", "install", lib, "-q"])

install_dependencies()

import requests
from bs4 import BeautifulSoup
import feedparser

# --- é…ç½®åŒº ---
FEISHU_WEBHOOK_URL = os.getenv('FEISHU_WEBHOOK_URL', '')
TODAY = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- è¾…åŠ©åŠŸèƒ½ï¼šå»é‡ ---
class NewsDeleter:
    def __init__(self):
        self.seen_titles = set()

    def is_duplicate(self, title: str) -> bool:
        # æ¸…æ´—æ ‡é¢˜ï¼ˆå»ç©ºæ ¼ã€å»ç¬¦å·ï¼‰
        clean_title = re.sub(r'[^\w\u4e00-\u9fa5]', '', title.lower())
        # å–å‰15ä¸ªå­—ç¬¦åšç®€æ˜“æŒ‡çº¹åŒ¹é…
        fingerprint = clean_title[:15]
        if fingerprint in self.seen_titles:
            return True
        self.seen_titles.add(fingerprint)
        return False

# --- æŠ“å–é€»è¾‘ ---

def fetch_rss_news(source_name: str, url: str) -> List[Dict]:
    """é€šç”¨çš„ RSS æŠ“å–é€»è¾‘"""
    news = []
    try:
        print(f"ğŸŒ æ­£åœ¨æŠ“å–å›½å¤–æº: {source_name}...")
        feed = feedparser.parse(url)
        for entry in feed.entries[:5]:
            news.append({
                "title": entry.title,
                "source": source_name,
                "summary": entry.get('summary', 'æŸ¥çœ‹åŸæ–‡').split('<')[0][:80] + "...",
                "link": entry.link
            })
    except Exception as e:
        print(f"âš ï¸ {source_name} æŠ“å–å¤±è´¥: {e}")
    return news

def fetch_36kr() -> List[Dict]:
    """æŠ“å– 36Kr AI é¢‘é“"""
    news = []
    try:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨æŠ“å–å›½å†…æº: 36æ°ª...")
        res = requests.get("https://36kr.com/information/ai/", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('.article-item-title-weight')
        for item in items[:5]:
            news.append({
                "title": item.get_text(strip=True),
                "source": "36æ°ª",
                "link": "https://36kr.com" + item['href']
            })
    except Exception as e:
        print(f"âš ï¸ 36Kr æŠ“å–å¤±è´¥: {e}")
    return news

def fetch_ithome() -> List[Dict]:
    """æŠ“å– ITä¹‹å®¶ AI æ ‡ç­¾"""
    news = []
    try:
        print("ğŸ‡¨ğŸ‡³ æ­£åœ¨æŠ“å–å›½å†…æº: ITä¹‹å®¶...")
        res = requests.get("https://www.ithome.com/tag/ai", headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.select('.news-item .title')
        for item in items[:5]:
            news.append({
                "title": item.get_text(strip=True),
                "source": "ITä¹‹å®¶",
                "link": item['href']
            })
    except Exception as e:
        print(f"âš ï¸ ITä¹‹å®¶ æŠ“å–å¤±è´¥: {e}")
    return news

# --- ä¸»é€»è¾‘èšåˆ ---

def get_all_news():
    duplicator = NewsDeleter()
    
    # 1. æŠ“å–æµ·å¤–ï¼ˆå¤šæºï¼‰
    overseas_sources = [
        {"name": "AI News", "url": "https://www.artificialintelligence-news.com/feed/"},
        {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/"},
        {"name": "The Verge AI", "url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml"}
    ]
    all_overseas = []
    for s in overseas_sources:
        raw_news = fetch_rss_news(s['name'], s['url'])
        for n in raw_news:
            if not duplicator.is_duplicate(n['title']):
                all_overseas.append(n)
    
    # 2. æŠ“å–å›½å†…ï¼ˆå¤šæºï¼‰
    all_domestic = []
    domestic_raw = fetch_36kr() + fetch_ithome()
    for n in domestic_raw:
        if not duplicator.is_duplicate(n['title']):
            all_domestic.append(n)
            
    return all_overseas[:8], all_domestic[:8] # å„è‡ªæˆªå–ç²¾é€‰ 8 æ¡

def generate_daily_report(overseas: List[Dict], domestic: List[Dict]) -> str:
    report = f"# ğŸ¤– AI å…¨ç½‘èšåˆæ—¥æŠ¥ - {TODAY}\n\n"
    
    report += "## ğŸ“° æµ·å¤–å¤´æ¡ (Multi-Source)\n\n"
    for i, n in enumerate(overseas, 1):
        report += f"### {i}. {n['title']}\n- æ¥æº: {n['source']}\n- é“¾æ¥: {n['link']}\n\n"
    
    report += "## ğŸ‡¨ğŸ‡³ å›½å†…åŠ¨æ€ (Multi-Source)\n\n"
    for i, n in enumerate(domestic, 1):
        report += f"### {i}. {n['title']}\n- æ¥æº: {n['source']}\n- é“¾æ¥: {n['link']}\n\n"
    
    report += f"---\n*Matrix Agent èšåˆæ£€ç´¢ | è¦†ç›–æº: 36Kr, ITä¹‹å®¶, TechCrunch, AI News, The Verge*"
    return report

def push_to_feishu(content: str):
    if not FEISHU_WEBHOOK_URL:
        print("âš ï¸ æœªé…ç½®é£ä¹¦ Webhook")
        return
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– AIå¤šæºæ—¥æŠ¥ - {TODAY}"}, "template": "purple"},
            "elements": [{"tag": "markdown", "content": content}]
        }
    }
    requests.post(FEISHU_WEBHOOK_URL, json=payload, timeout=20)

def main():
    print(f"ğŸš€ å¯åŠ¨å¤šæºæƒ…æŠ¥æŠ“å–ä»»åŠ¡...")
    overseas, domestic = get_all_news()
    if not overseas and not domestic:
        print("âŒ æœªè·å–åˆ°ä»»ä½•èµ„è®¯")
        return
    report = generate_daily_report(overseas, domestic)
    push_to_feishu(report)
    print("âœ… æ—¥æŠ¥æ¨é€å®Œæˆ")

if __name__ == "__main__":
    main()
